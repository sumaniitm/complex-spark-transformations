{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3148a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/20 22:33:59 WARN Utils: Your hostname, Sumans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "21/10/20 22:33:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/10/20 22:34:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.2 (default, Apr  8 2021 23:19:18)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd18a82",
   "metadata": {},
   "source": [
    "#### First let's read in the processed and cleansed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ba21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile('/Users/sumangangopadhyay/complex-spark-transformations/config.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50792855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cf\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44dbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_path = cf.clean_data_path()\n",
    "primary_response_variables = cf.primary_response_variables().split(',')\n",
    "secondary_response_variables = cf.secondary_response_variables().split(',')\n",
    "primary_explanatory_variables = cf.primary_explanatory_variables().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4a6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(clean_data_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeefc49",
   "metadata": {},
   "source": [
    "#### Let us look for seasonality, i.e. if the incidents of violation are more in particular month(s). Note how the Issue_Month column is sorted using type casting, since this column wasn't derived as an integer before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05fa1b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/20 22:34:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:===>                                                     (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|Issue_Month|  count|\n",
      "+-----------+-------+\n",
      "|          1| 877365|\n",
      "|          2| 826967|\n",
      "|          3| 964737|\n",
      "|          4| 888402|\n",
      "|          5|1020244|\n",
      "|          6| 852187|\n",
      "|          7|    370|\n",
      "|          8|    309|\n",
      "|          9|    367|\n",
      "|         10|    274|\n",
      "|         11|    338|\n",
      "|         12|    358|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.Issue_Month).count().orderBy(df.Issue_Month.cast(IntegerType())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b2ee4",
   "metadata": {},
   "source": [
    "#### It is quite clear that the violations are most common during the first half of the year, so let's dig deeper in the first half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55057522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first6months = df.filter((df.Issue_Month.cast(IntegerType()) >= 1) \\\n",
    "                           | (df.Issue_Month.cast(IntegerType()) <= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95b4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_first6months.groupBy(df_first6months.Issue_DayofMonth).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3666ba89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Issue_DayofMonth', 'count']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0f4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.createOrReplaceTempView('t_count_per_day_of_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4242958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_standardised = spark.sql(\"select t1.Issue_DayofMonth, t1.count, \\\n",
    "round((t1.count - t2.avg_issue_count)/t2.stddev_issue_count,2) as standardised_issue_count \\\n",
    "from t_count_per_day_of_month t1 \\\n",
    "join (select avg(count) as avg_issue_count, stddev(count) as stddev_issue_count \\\n",
    "from t_count_per_day_of_month)t2 on 1=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62250b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------------------------+\n",
      "|Issue_DayofMonth| count|standardised_issue_count|\n",
      "+----------------+------+------------------------+\n",
      "|               1|192311|                    0.53|\n",
      "|               2|195915|                    0.64|\n",
      "|               3|225398|                    1.56|\n",
      "|               4|179977|                    0.15|\n",
      "|               5|162463|                    -0.4|\n",
      "|               6|211419|                    1.13|\n",
      "|               7|179266|                    0.13|\n",
      "|               8|187606|                    0.39|\n",
      "|               9|170336|                   -0.15|\n",
      "|              10|182476|                    0.23|\n",
      "|              11|166438|                   -0.27|\n",
      "|              12|162547|                    -0.4|\n",
      "|              13|202336|                    0.84|\n",
      "|              14|140262|                   -1.09|\n",
      "|              15|171100|                   -0.13|\n",
      "|              16|190189|                    0.47|\n",
      "|              17|195997|                    0.65|\n",
      "|              18|160471|                   -0.46|\n",
      "|              19|170672|                   -0.14|\n",
      "|              20|216504|                    1.29|\n",
      "|              21|190748|                    0.48|\n",
      "|              22|170821|                   -0.14|\n",
      "|              23|201773|                    0.83|\n",
      "|              24|207561|                    1.01|\n",
      "|              25|154025|                   -0.66|\n",
      "|              26|158211|                   -0.53|\n",
      "|              27|216891|                     1.3|\n",
      "|              28|153615|                   -0.67|\n",
      "|              29| 81421|                   -2.92|\n",
      "|              30|131880|                   -1.35|\n",
      "|              31|101289|                    -2.3|\n",
      "+----------------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_standardised.orderBy(df_temp_standardised.Issue_DayofMonth.cast(IntegerType())).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143ea81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
